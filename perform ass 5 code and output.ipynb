{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1693c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the CBOW model, the distributed representations of context (or surrounding words) are combined to predict the word in the middle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d09c326a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://thinkinfi.com/continuous-bag-of-words-cbow-single-word-model-how-it-works/\n",
    "\n",
    "# 1. Data Preparation:\n",
    "# Let’s say we have a text like below:\n",
    " \n",
    "# “i like natural language processing”\n",
    " \n",
    "# To make it simple I have chosen a sentence without capitalization and punctuation. Also I will not remove any stop words (“and”, “the” etc.) but for real world implementation you should do lots of cleaning task like stop word removal, replacing digits, remove punctuation etc.\n",
    " \n",
    "# After pre-processing we will convert the text to list of tokenized word.\n",
    " \n",
    "# [“i”, “like”, “natural”, “language”, “processing”]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4cde6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Generate training data:\n",
    "# Unique vocabulary: Find unique vocabulary list. As we don’t have any duplicate word in our example text, so unique vocabulary will be:\n",
    " \n",
    "# [“i”, “like”, “natural”, “language”, “processing”]\n",
    " \n",
    "# Now to prepare training data for single word CBOW model, we define “target word” as the word which follows a given word in the text (which will be our “context word”). That means we will be predicting next word for a given word.\n",
    " \n",
    "# Now let’s construct our training examples, scanning through the text with a window will prepare a context word and a target word, like so:\n",
    "\n",
    "\n",
    "\n",
    "# For example, for context word “i” the target word will be “like”. For our example text full training data will looks like:\n",
    "\n",
    "# One-hot encoding: We need to convert text to one-hot encoding as algorithm can only understand numeric values.\n",
    " \n",
    "# For example encoded value of the word “i”, which appears first in the vocabulary, will be as the vector [1, 0, 0, 0, 0]. The word “like”, which appears second in the vocabulary, will be encoded as the vector [0, 1, 0, 0, 0]\n",
    " \n",
    "\n",
    "\n",
    " \n",
    "\n",
    "# So let’s see overall set of context-target words in one hot encoded form\n",
    " \n",
    "\n",
    " \n",
    "# So as you can see above table is our final training data, where encoded target word is Y variablefor our model and encoded context word is X variable for our model.\n",
    " \n",
    "# Now we will move on to train our model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9664c6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Training Model:\n",
    " \n",
    "\n",
    "# So far, so good right? Now we need to pass this data into the basic neural network with one hidden layer and train it. Only one thing to note is that the desire vector dimension of any word will be the number of hidden nodes.\n",
    " \n",
    "# For this tutorial and demo purpose my desired vector dimension is 3. For example:\n",
    " \n",
    "# “i” => [0.001, 0.896, 0.763]  so number of hidden layer node will be 3.\n",
    " \n",
    "# Dimension (n): It is dimension of word embedding you can treat embedding as number of features or entity like organization, name, gender etc. It can be 10, 20, 100 etc. Increasing number of embedding layer will explain a word or token more deeply. Just for an example Google pre-trained word2vec have dimension of 300.\n",
    " \n",
    "# Now as you know a basic neural network training is divided into some steps:\n",
    "# 1. Create model Architecture\n",
    "# 2. Forward Propagation\n",
    "# 3. Error Calculation\n",
    "# 4. Weight tuning using backward pass\n",
    "# Before going into forward propagation we need to understand model architecture in vectorized form.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e625d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7bb2b0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37550c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pylab as pylab\n",
    "import numpy as np\n",
    "# %matplotlib inline\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "478ee836",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = \"\"\"we are about to study\"\"\"\n",
    "# the idea of a computational process.\n",
    "#  Computational processes are abstract beings that inhabit computers.\n",
    "#  As they evolve, processes manipulate other abstract things called data.\n",
    "#  The evolution of a process is directed by a pattern of rules\n",
    "#  called a program. People create programs to direct processes. In effect,\n",
    "#  we conjure the spirits of the computer with our spells.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ccd8d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove special characters\n",
    "# sentences = re.sub('[^A-Za-z0-9]+', ' ', sentences)\n",
    "\n",
    "# remove 1 letter words\n",
    "# sentences = re.sub(r'(?:^| )\\w(?:$| )', ' ', sentences).strip()\n",
    "\n",
    "# lower all characters\n",
    "# sentences = sentences.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95daef46",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcf5fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vocabulary\n",
    "\n",
    "words = sentences.split()\n",
    "vocab = set(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d415698",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "embed_dim = 10\n",
    "context_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebeb112c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Impleentation\n",
    "\n",
    "word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
    "ix_to_word = {i: word for i, word in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1364917c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data - [(context), target]\n",
    "\n",
    "data = []\n",
    "for i in range(2, len(words) - 2):\n",
    "    context = [words[i - 2], words[i - 1], words[i + 1], words[i + 2]]\n",
    "    target = words[i]\n",
    "    data.append((context, target))\n",
    "print(data[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598991dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings =  np.random.random_sample((vocab_size, embed_dim))\n",
    "\n",
    "# Linear Model\n",
    "\n",
    "def linear(m, theta):\n",
    "    w = theta\n",
    "    return m.dot(w)\n",
    "\n",
    "# Log softmax + NLLloss = Cross Entropy\n",
    "\n",
    "def log_softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return np.log(e_x / e_x.sum())\n",
    "def NLLLoss(logs, targets):\n",
    "    out = logs[range(len(targets)), targets]\n",
    "    return -out.sum()/len(out)\n",
    "def log_softmax_crossentropy_with_logits(logits,target):\n",
    "\n",
    "    out = np.zeros_like(logits)\n",
    "    out[np.arange(len(logits)),target] = 1\n",
    "    \n",
    "    softmax = np.exp(logits) / np.exp(logits).sum(axis=-1,keepdims=True)\n",
    "    \n",
    "    return (- out + softmax) / logits.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0a8092",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward function\n",
    "\n",
    "def forward(context_idxs, theta):\n",
    "    m = embeddings[context_idxs].reshape(1, -1)\n",
    "    n = linear(m, theta)\n",
    "    o = log_softmax(n)\n",
    "    \n",
    "    return m, n, o\n",
    "\n",
    "# Backward function\n",
    "\n",
    "def backward(preds, theta, target_idxs):\n",
    "    m, n, o = preds\n",
    "    \n",
    "    dlog = log_softmax_crossentropy_with_logits(n, target_idxs)\n",
    "    dw = m.T.dot(dlog)\n",
    "    \n",
    "    return dw\n",
    "\n",
    "# Optimize function\n",
    "\n",
    "def optimize(theta, grad, lr=0.03):\n",
    "    theta -= grad * lr\n",
    "    return theta\n",
    "\n",
    "\n",
    "# Training \n",
    "\n",
    "theta = np.random.uniform(-1, 1, (2 * context_size * embed_dim, vocab_size))\n",
    "epoch_losses = {}\n",
    "\n",
    "for epoch in range(80):\n",
    "\n",
    "    losses =  []\n",
    "\n",
    "    for context, target in data:\n",
    "        context_idxs = np.array([word_to_ix[w] for w in context])\n",
    "        preds = forward(context_idxs, theta)\n",
    "\n",
    "        target_idxs = np.array([word_to_ix[target]])\n",
    "        loss = NLLLoss(preds[-1], target_idxs)\n",
    "\n",
    "        losses.append(loss)\n",
    "\n",
    "        grad = backward(preds, theta, target_idxs)\n",
    "        theta = optimize(theta, grad, lr=0.03)\n",
    "        \n",
    "     \n",
    "    epoch_losses[epoch] = losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db909466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze \n",
    "# Plot loss/epoch\n",
    "\n",
    "ix = np.arange(0,80)\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.suptitle('Epoch/Losses', fontsize=20)\n",
    "plt.plot(ix,[epoch_losses[i][0] for i in ix])\n",
    "plt.xlabel('Epochs', fontsize=12)\n",
    "plt.ylabel('Losses', fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896ac035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict function\n",
    "\n",
    "def predict(words):\n",
    "    context_idxs = np.array([word_to_ix[w] for w in words])\n",
    "    preds = forward(context_idxs, theta)\n",
    "    word = ix_to_word[np.argmax(preds[-1])]\n",
    "    \n",
    "    return word\n",
    "# (['we', 'are', 'to', 'study'], 'about')\n",
    "predict(['we', 'are', 'to', 'study'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8635e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "\n",
    "def accuracy():\n",
    "    wrong = 0\n",
    "\n",
    "    for context, target in data:\n",
    "        if(predict(context) != target):\n",
    "            wrong += 1\n",
    "            \n",
    "    return (1 - (wrong / len(data)))\n",
    "\n",
    "\n",
    "accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e7d0be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f41b9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
